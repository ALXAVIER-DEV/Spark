{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Structured Streaming",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Aula_9_Structured_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "outputId": "228e1d3b-7704-45c0-a15a-ec0956469135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (65.8.186.116)] [Wai\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [3 InRelease 3,626 B/3,626 \u001b[0m\u001b[33m\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,688 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.4 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,130 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [252 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,208 kB]\n",
            "Fetched 7,469 kB in 3s (2,451 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "18 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=74764f11bc7407744f7efc7bfba202d6c1f93a726def299188ac3b8e896e413e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxNTEvOQMKI7"
      },
      "source": [
        "# Structured Streaming - Introdução\n",
        "O objetivo desse tópico é mostrar uma introdução ao Structured Streaming e como utilizá-lo em conjunto com uma source Kafka. Vamos ver como ler e exibir os dados recebidos em real-time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2CGKD-KZFP-"
      },
      "source": [
        "## Lendo o dado bruto do Kafka\n",
        "Ao configurar o Spark Session, basta passar como parâmetro a config relacionada ao package do kafka"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aKclVCoLu0J"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Teste\")\\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1')\\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWdNvQ-PZsqN"
      },
      "source": [
        "Criando o dataframe a partir do Kafka. Observe os parâmetros obrigatórios: **url do broker kafka** e o **nome do tópico**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw2DQZ-NZY96"
      },
      "source": [
        "df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"13.82.0.139:9092\") \\\n",
        "  .option(\"subscribe\", \"queueing.transactions\") \\\n",
        "  .load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh3RwyFjZ_fc"
      },
      "source": [
        "Printando o schema do dado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbwDe8YPZcsF",
        "outputId": "96924971-6c58-4f8f-a245-e3e274f8fbb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Z-QPE8aBzs"
      },
      "source": [
        "Escrevendo o Dataframe na memória, no formato de uma tabela temporária (utilizado apenas em ambiente de testes ou debug)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q1mNmKQaE8E"
      },
      "source": [
        "query = df \\\n",
        "  .writeStream \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"transactions_raw\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M3P7oSlafEj"
      },
      "source": [
        "Vendo o dado recebido através do SparkSQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpUbvLsjada8",
        "outputId": "f0a659b7-1ab2-4ed1-f4ee-cd696a72d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT count(*) FROM transactions_raw\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|    9760|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB3AUWjabpzF"
      },
      "source": [
        "IMPORTANTE!!! Ao final do streaming, é necessário parar a query com o comando abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMBLTUv6UcRy",
        "outputId": "098651f4-4456-4db7-b187-459e9ded2c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "query.lastProgress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batchId': 456,\n",
              " 'durationMs': {'addBatch': 34,\n",
              "  'getBatch': 0,\n",
              "  'latestOffset': 14,\n",
              "  'queryPlanning': 7,\n",
              "  'triggerExecution': 104,\n",
              "  'walCommit': 27},\n",
              " 'id': 'f9d4b4c5-8a87-408e-98bd-35a8b610e710',\n",
              " 'inputRowsPerSecond': 833.3333333333334,\n",
              " 'name': 'transactions_raw',\n",
              " 'numInputRows': 20,\n",
              " 'processedRowsPerSecond': 192.30769230769232,\n",
              " 'runId': '246ce09c-0eda-4377-827f-14459597a1ef',\n",
              " 'sink': {'description': 'MemorySink', 'numOutputRows': 20},\n",
              " 'sources': [{'description': 'KafkaV2[Subscribe[queueing.transactions]]',\n",
              "   'endOffset': {'queueing.transactions': {'0': 1433787}},\n",
              "   'inputRowsPerSecond': 833.3333333333334,\n",
              "   'numInputRows': 20,\n",
              "   'processedRowsPerSecond': 192.30769230769232,\n",
              "   'startOffset': {'queueing.transactions': {'0': 1433767}}}],\n",
              " 'stateOperators': [],\n",
              " 'timestamp': '2020-11-16T23:11:09.583Z'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPvJMaTbpgz"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ABwG7abNVk"
      },
      "source": [
        "## Lendo o dado de forma legível (com schema)\n",
        "Antes de iniciarmos, vamos definir um schema de acordo com o dado gerado pelo gerador de transações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOfi8CruMQ1d"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"source\", StringType(), True),\n",
        "    StructField(\"target\", StringType(), True),\n",
        "    StructField(\"amount\", DoubleType(), True),\n",
        "    StructField(\"x\", IntegerType(), True),\n",
        "    StructField(\"y\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"currency\", StringType(), True)\n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWESat5blTM"
      },
      "source": [
        "Agora, criaremos o Dataframe utilizando o Kafka como source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xaZ8Tgtbkqo"
      },
      "source": [
        "df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"13.82.0.139:9092\") \\\n",
        "  .option(\"subscribe\", \"queueing.transactions\") \\\n",
        "  .load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jojVwDxFb5i7"
      },
      "source": [
        "Utilizaremos agora o schema criado previamente para traduzir o dado binário em um dado legível, dado o schema criado anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBebrngGcBhj"
      },
      "source": [
        "identified_df = df.select(f.from_json(f.col(\"value\").cast(\"string\"), schema).alias(\"json\")) \\\n",
        "  .selectExpr(\n",
        "      \"json.source\", \n",
        "      \"json.target\",\n",
        "      \"json.amount\",\n",
        "      \"json.x\",\n",
        "      \"json.y\",\n",
        "      \"json.timestamp\",\n",
        "      \"json.currency\"\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1xpKhOmcVd3"
      },
      "source": [
        "Printando o schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqUtNnM9V9Cw"
      },
      "source": [
        "identified_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCq-rmwhcYSL"
      },
      "source": [
        "Escrevendo o dataframe na memória, em formato de tabela temporária e visualizando os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQPiMo_8Q_Ab"
      },
      "source": [
        "query = identified_df \\\n",
        "  .writeStream \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"transactions\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc39tNjMcdyv"
      },
      "source": [
        "def display_df(df, limit=20):\n",
        "  return df.limit(limit).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqec9ZHhRe3f"
      },
      "source": [
        "display_df(spark.sql(\"SELECT count(*) FROM transactions\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsDDGudNR_5P"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbuw4yCp1G9x"
      },
      "source": [
        "Veja mais sources disponíveis no Spark Structured Streaming em: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uE8Un5geRQZ"
      },
      "source": [
        "# Transformações básicas\n",
        "Vamos decorrer essa seção executando algumas transformações básicas para agregar e exibir dados. Obs: considere a sigla NRT = Near Real Time\n",
        "\n",
        "1) Quantidade de transações por source em NRT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQHwcoCmeSxZ"
      },
      "source": [
        "qnt_sources_df = identified_df \\\n",
        "  .groupBy(\"source\") \\\n",
        "  .count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y0N_EehfdAi"
      },
      "source": [
        "query = qnt_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"qnt_sources\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0joKsw0Rf7_a"
      },
      "source": [
        "display_df(spark.sql(\"\"\"\n",
        "SELECT * FROM qnt_sources\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHx_AiTgOS9"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfM5sVgYhCEn"
      },
      "source": [
        "2) O valor total movimentado por cada source começada com a letra **s**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJZFJKSg0S7"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "qnt_s_sources_df = identified_df \\\n",
        "  .groupBy(\"source\") \\\n",
        "  .agg(f.sum(\"amount\").alias(\"total_amount\")) \\\n",
        "  .where(\"UPPER(source) LIKE 'S%'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98_ufFFahjP8"
      },
      "source": [
        "query = qnt_s_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"qnt_s_sources\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKiLdTfJht5C"
      },
      "source": [
        "display_df(spark.sql(\"\"\"\n",
        "SELECT * FROM qnt_s_sources\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WlWj4d8hwkq"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAv_4o4rikUe"
      },
      "source": [
        "# Operações em Windows utilizando o conceito de event-time\n",
        "Vamos ver agora como funcionam as operações em Janelas temporais (windows). Como explicação, considere o exemplo retirado da documentação do Spark Structured Streaming:\n",
        "\n",
        "![](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n",
        "\n",
        "\n",
        "Vamos contar a quantidade de transações dentro de uma janela de 30 segundos, deslizante a cada 5 segundos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JvrV9Goh8BP"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "windowed_count = identified_df \\\n",
        "  .groupBy(\n",
        "      f.window(\"timestamp\", \"30 seconds\", \"5 seconds\")\n",
        "   ) \\\n",
        "  .count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwm_Rx5rk6N"
      },
      "source": [
        "query = windowed_count \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"windowed_count\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG9QEUtRr0rW"
      },
      "source": [
        "display_df(spark.sql(\"\"\"\n",
        "SELECT * FROM windowed_count\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCyH4PsltzmH"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT * FROM windowed_count\n",
        "\"\"\").printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71pbbfDDr5ND"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBsNRNhTyfP8"
      },
      "source": [
        "Manuseando eventos atrasados com os **Watermark**. Veja mais em https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxfXf62GsKM0"
      },
      "source": [
        "windowed_watermark_count = identified_df \\\n",
        "  .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
        "  .groupBy(\n",
        "      f.window(\"timestamp\", \"30 seconds\", \"5 seconds\")\n",
        "   ) \\\n",
        "  .count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvNvGxLK3Hdq"
      },
      "source": [
        "# Realizando a saída dos resultados\n",
        "\n",
        "Sempre que o Structured Streaming realizar alguma operação de output dos dados, isso será feito em uma **sink**, ou seja, em um local fora do contexto do Spark.\n",
        "\n",
        "Além disso, após definir o conjunto de transformações a qual o dado passará, é necessário colocar a query em streaming para executar e salvar os dados em algum local onde possam ser consumidos. Isso se dá através do método **writeStream**. Assim sendo, você pode especificar os seguintes parâmetros:\n",
        "- Detalhes da output sink: formato, header, etc.\n",
        "- Output mode: especifica o que será escrito na output sink.\n",
        "- Query name: Opcional, um identificador único para a tabela, e assim poder realizar queries em cima do dado em memória.\n",
        "- Trigger Interval:  Opcional; refere-se ao momento que ocorrerá a saída dos dados.\n",
        "- Checkpoint location: local onde será salvo o checkpoint para garantir entrega end-to-end para algumas sinks.\n",
        "\n",
        "Veja mais sobre os tipos de Output modes e suas implicações em determinadas queries no link https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes.\n",
        "\n",
        "Veja também sobre as triggers disponíveis para uso: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\n",
        "\n",
        "Vejamos agora algumas das sinks disponíveis para uso:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayyOEnq67l37"
      },
      "source": [
        "## File Sink"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUhB0F3c7oOb"
      },
      "source": [
        "Modo append e sem trigger interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqYgawRA3YHn"
      },
      "source": [
        "qnt_sources_df = identified_df \\\n",
        "  .selectExpr(\"source\", \"amount\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPlvsFpn3nUa"
      },
      "source": [
        "query = qnt_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"append\") \\\n",
        "  .format(\"csv\") \\\n",
        "  .option(\"path\", \"output/streaming_csv\") \\\n",
        "  .option(\"checkpointLocation\", \"checkpoint\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUOPvawW34Mo"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhQLslKI7ios"
      },
      "source": [
        "Modo append e com trigger interval de 2 minutos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42MeQg0K4maW"
      },
      "source": [
        "query = qnt_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"append\") \\\n",
        "  .trigger(processingTime='2 minutes') \\\n",
        "  .format(\"csv\") \\\n",
        "  .option(\"path\", \"output/streaming_csv\") \\\n",
        "  .option(\"checkpointLocation\", \"checkpoint_file\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH7vlqCl8Wye"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5qPhgk9Lzs"
      },
      "source": [
        "## Kafka Sink\n",
        "\n",
        "Modo complete x modo update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlpy1mRP9WLe"
      },
      "source": [
        "qnt_sources_df = identified_df \\\n",
        "  .groupBy(\"source\") \\\n",
        "  .count() \\\n",
        "  .selectExpr(\"CAST(source AS STRING) AS key\", \"CAST(count AS STRING) AS value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naX9mBY_9WLj"
      },
      "source": [
        "# trocar o modo para update\n",
        "\n",
        "query = qnt_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"13.82.0.139:9092\") \\\n",
        "  .option(\"topic\", \"queueing.transactions.out\")\\\n",
        "  .option(\"checkpointLocation\", \"checkpoint_kafka\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxrmayQi9kwz"
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyAmqPp7BcTA"
      },
      "source": [
        "# Monitorando as suas queries de streaming ativas\n",
        "\n",
        "Em alguns momentos, é interessante que você tenha acesso a todas as queries em execução naquele exato momento. Manter muitas conexões ativas pode ser prejudicial tanto para a estação que está executando as queries, quanto para o broker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbDizljC_Phf"
      },
      "source": [
        "spark.streams.active"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yc7DgewDzkG"
      },
      "source": [
        "Para parar uma das seções ativas do comando anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJo1DhbNBGsn"
      },
      "source": [
        "spark.streams.active[0].stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wptLD2YXETQn"
      },
      "source": [
        "Veja uma lista de ações passíveis de serem realizadas utilizando o monitoramento das queries:\n",
        "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#managing-streaming-queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T609Lnb6EsRc"
      },
      "source": [
        "Exercícios\n",
        "\n",
        "Utilizando os dados de transações financeiras coletados no broker Kafka, realize as seguintes queries em streaming:\n",
        "\n",
        "1) Retornar a source, o valor da transação e o imposto pago, considerando 10% do valor da transação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XjlTjX1Eten"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfVCCUWjFCzd"
      },
      "source": [
        "2) Implemente um contador de transações em streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnWNlbhTFL_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2cqggLFMcf"
      },
      "source": [
        "3) Implemente um contador de transações utilizando uma janela de 2 minutos e um slide interval de 20 segundos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wH0954QFU4b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}