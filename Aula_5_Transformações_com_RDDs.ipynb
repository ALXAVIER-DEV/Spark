{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Aula 5 - Transformações com RDDs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Aula_5_Transforma%C3%A7%C3%B5es_com_RDDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "outputId": "68736189-b0a8-4b80-8f41-5409a9cd2b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pXEN-M0hfV",
        "outputId": "e09b2b17-08b4-4f40-b877-c03f31fee4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7e2cd20706e5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7dc872ab70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l57w6j1HrcGr"
      },
      "source": [
        "# Reading a CSV from google drive\n",
        "\n",
        "Utilizando o Google Colab, é possível importar os datasets diretamente do Google Drive, sem ter que realizar o upload manual dos mesmos para a instância colab manualmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIlwwr7xENz",
        "outputId": "149de23a-af34-4c87-f2da-40c50a3280ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LB-SAxzAFr",
        "outputId": "1297fed9-6163-462d-db5c-6089ce4c72a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "spark.read\\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"delimiter\", \",\") \\\n",
        "  .csv(\"drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_test/test.csv\") \\\n",
        "  .show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-09999708fd97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_test/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_test/test.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD40kE_BM5UD"
      },
      "source": [
        "# Transformações com RDDs\n",
        "Nessa aula veremos algumas das mais comuns transformações utilizanod RDDs. Lembrando que todas as transformações High-level realizadas em dataframes são convertidas em transformações low-level utilizando os RDDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySCsDpjTPx2w"
      },
      "source": [
        "sc = spark.sparkContext"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNJUZBNlM5UE"
      },
      "source": [
        "### Coleção a ser utilizada\n",
        "Durante essa parte do curso, veremos como aplicar os comandos utilizando a seguinte coleção em Python (texto extraído da seção Overwiew em http://spark.apache.org/docs/latest/rdd-programming-guide.html#overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZhcNUKXM5UF"
      },
      "source": [
        "text = \"At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures. A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only added to, such as counters and sums\"\n",
        "words = text.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eri84evmM5UI"
      },
      "source": [
        "### Criando o RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi9W4wqOM5UI",
        "outputId": "8488efd8-9826-42ee-da01-1c142ba73a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Look for the core number\n",
        "rdd = sc.parallelize(words, 4)\n",
        "rdd.setName(\"myWords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "myWords ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJJP4zD2M5UL",
        "outputId": "98a8e6fa-2665-4c73-d15d-597f1cf33811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['At',\n",
              " 'a',\n",
              " 'high',\n",
              " 'level,',\n",
              " 'every',\n",
              " 'Spark',\n",
              " 'application',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJopZP68M5UO"
      },
      "source": [
        "### Transformações\n",
        "##### rdd.map()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.map(<FUNCAO>)\n",
        "```\n",
        "A transformação `map()` irá mapear cada linha de `rdd1` em uma linha em `rdd2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urYwMMnRM5UO"
      },
      "source": [
        "''''\n",
        "Cada linha do rdd irá ser mapeada para uma nova linha contendo \n",
        "a palavra, o tamanho da palavra e se começa com a letra 'S'\n",
        "'''\n",
        "def starts_with_S(word):\n",
        "    return word.startswith(\"S\")\n",
        "\n",
        "new_rdd = rdd.map(lambda word: (word, len(word), starts_with_S(word)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vjOHqA5pM5UQ",
        "outputId": "2195299c-80c9-4857-9f09-7e658e5bace0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_rdd.take(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('At', 2, False),\n",
              " ('a', 1, False),\n",
              " ('high', 4, False),\n",
              " ('level,', 6, False),\n",
              " ('every', 5, False),\n",
              " ('Spark', 5, True),\n",
              " ('application', 11, False),\n",
              " ('consists', 8, False),\n",
              " ('of', 2, False),\n",
              " ('a', 1, False),\n",
              " ('driver', 6, False),\n",
              " ('program', 7, False),\n",
              " ('that', 4, False),\n",
              " ('runs', 4, False),\n",
              " ('the', 3, False),\n",
              " ('user’s', 6, False),\n",
              " ('main', 4, False),\n",
              " ('function', 8, False),\n",
              " ('and', 3, False),\n",
              " ('executes', 8, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVNkFFLXM5US"
      },
      "source": [
        "##### rdd.filter()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.filter(<FUNCAO>)\n",
        "```\n",
        "A transformação `filter()` irá remover linhas que resultem em `False` quando aplicadas à `<FUNCAO>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S974jws3M5UT"
      },
      "source": [
        "''''\n",
        "Todas as linhas cuja posição 2 seja False serão removidas\n",
        "'''\n",
        "filtered_rdd = new_rdd.filter(lambda row: row[2]) # ou row[2] == True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufDDB8SpM5UV",
        "outputId": "25d48408-8df0-4320-82bd-9abdb6d68ffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filtered_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Spark', 5, True),\n",
              " ('Spark', 5, True),\n",
              " ('Scala', 5, True),\n",
              " ('Spark', 5, True),\n",
              " ('Spark', 5, True),\n",
              " ('Spark', 5, True),\n",
              " ('Sometimes,', 10, True),\n",
              " ('Spark', 5, True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vcwsNtYM5UY"
      },
      "source": [
        "##### rdd.flatMap()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.flatMap(<FUNCAO>)\n",
        "```\n",
        "A transformação `flatMap()` irá mapear cada linha de `rdd1` em uma ou mais linhas em `rdd2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK6OOS7EVI_P",
        "outputId": "5429e541-428a-41cc-92dd-8146ce1a7bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(\"igor\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'g', 'o', 'r']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYVFEYCNM5UY"
      },
      "source": [
        "''''\n",
        "Todas as palavras do rdd filtrado serão convertidas em um RDD contendo apenas\n",
        "letras\n",
        "'''\n",
        "\n",
        "flatted_rdd = filtered_rdd.flatMap(lambda row: list(row[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "PFYyfmygM5Ua",
        "outputId": "9c626c71-6b2e-41e0-f5dc-59964b9b867e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "flatted_rdd.take(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['S',\n",
              " 'p',\n",
              " 'a',\n",
              " 'r',\n",
              " 'k',\n",
              " 'S',\n",
              " 'p',\n",
              " 'a',\n",
              " 'r',\n",
              " 'k',\n",
              " 'S',\n",
              " 'c',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " 'S',\n",
              " 'p',\n",
              " 'a',\n",
              " 'r',\n",
              " 'k']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqIzt35dM5Uc"
      },
      "source": [
        "##### rdd.distinct()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.distinct()\n",
        "```\n",
        "A transformação `distinct()` irá gerar um novo rdd com apenas objetos distintos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNCqphRkM5Ud"
      },
      "source": [
        "''''\n",
        "Obteremos a lista de palavras únicas na frase\n",
        "'''\n",
        "\n",
        "distinct_rdd = rdd.distinct()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bxnLUsAJM5Ug",
        "outputId": "4a4ae0e7-5083-47b0-9120-67e10af21245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "distinct_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['At',\n",
              " 'of',\n",
              " 'operations',\n",
              " 'provides',\n",
              " 'in',\n",
              " 'are',\n",
              " 'other',\n",
              " 'Hadoop-supported',\n",
              " 'an',\n",
              " 'may']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqkqcKtjM5Ul",
        "outputId": "d77d14e8-93c4-4a21-bd95-b21e2991811f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "distinct_rdd.count()\n",
        "#rdd.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsVAFr0M5Un"
      },
      "source": [
        "##### rdd.sortBy()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.sortBy(<FUNCAO>)\n",
        "```\n",
        "A transformação `sortBy()` irá ordenar as linhas do rdd1 conforme critério definido na `<FUNCAO>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVDKxs4M5Uo"
      },
      "source": [
        "''''\n",
        "Todas as palavras do rdd serão ordenadas conforme o tamanho da palavra\n",
        "'''\n",
        "\n",
        "sorted_distinct_rdd = distinct_rdd.sortBy(lambda word: len(word) * -1) # -1 significa order reversa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GS1YDPyqM5Uq",
        "outputId": "94acf5d8-26a5-4bca-c5cc-e5e2a1c14b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sorted_distinct_rdd.take(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hadoop-supported',\n",
              " 'accumulators,',\n",
              " 'automatically',\n",
              " 'transforming',\n",
              " 'operations.',\n",
              " 'partitioned',\n",
              " 'application',\n",
              " 'abstraction',\n",
              " 'distributed',\n",
              " 'efficiently',\n",
              " 'operations',\n",
              " 'variables,',\n",
              " 'Sometimes,',\n",
              " 'variables:',\n",
              " 'collection',\n",
              " 'failures.',\n",
              " 'different',\n",
              " 'broadcast',\n",
              " 'resilient',\n",
              " 'parallel.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COuLfROdM5Us"
      },
      "source": [
        "### Pair RDDs\n",
        "Alguns RDDs específicos no Spark permitem a aplicação de transformações de modo mais eficiente. Estes são chamados de PairRDDs porque se apresentam na forma de um rdd key-value, ou seja, (K,V). As transformações disponíveis para serem aplicadas nos PairRDDs normalmente segue o padrão `<some-operation>ByKey`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJ6vtM2M5Ut"
      },
      "source": [
        "##### Criando um PairRDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNBZRNiNM5Ut"
      },
      "source": [
        "rdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpwN1wyM5Uv"
      },
      "source": [
        "pair_rdd = rdd.map(lambda x: (x, 1))\n",
        "pair_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmR4Fmo-M5Ux"
      },
      "source": [
        "##### Criando um PairRDD utilizanod o keyBy\n",
        "É uma transformação que cria um PairRDD passando-se uma função para retornar qual será a key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs5mDfIZM5Uy"
      },
      "source": [
        "key_by_pair_rdd = rdd.keyBy(lambda x: x[0].lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ7NAyG_M5U0"
      },
      "source": [
        "key_by_pair_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LidLcFEDM5U1"
      },
      "source": [
        "##### rdd.mapValues()\n",
        "Assumindo que temos:\n",
        "```python\n",
        "rdd2 = rdd1.mapValues(<FUNCAO>)\n",
        "```\n",
        "Transformação aplicada somente no valor de um PairRDD, aplicando uma `<FUNCAO>` e transformando cada linha do rdd1 em outra linha no rdd2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjwRb-k1M5U2"
      },
      "source": [
        "letters = key_by_pair_rdd.mapValues(lambda x: 1)\n",
        "letters.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpnNc1quM5U4"
      },
      "source": [
        "##### rdd.reduceByKey\n",
        "Realiza uma operação de reduce em um PairRDD baseado em seus valores agrupando por key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWAxSsK2M5U4"
      },
      "source": [
        "# Realizando a contagem das palavras\n",
        "reduced_pair_rdd = pair_rdd.reduceByKey(lambda a, b: a + b)\n",
        "reduced_pair_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "57-Dt-SmM5U6"
      },
      "source": [
        "# Realizando a contagem que palavras que começam com as letras do alfabeto\n",
        "reduced_letters = letters.reduceByKey(lambda a, b: a + b)\n",
        "reduced_letters.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOuw2J7NM5U9"
      },
      "source": [
        "##### rdd.sortByKey\n",
        "Realiza uma operação de sort em um PairRDD baseado em sua key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzZ_WMJrM5U9"
      },
      "source": [
        "sorted_reduced_rdd = reduced_pair_rdd.sortByKey(ascending=False)\n",
        "sorted_reduced_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YdJJzV8TM5VA"
      },
      "source": [
        "sorted_reduced_letters = reduced_letters.sortByKey(ascending=False)\n",
        "sorted_reduced_letters.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bckNFcqQM5VD"
      },
      "source": [
        "##### rdd.join()\n",
        "Transformação responsável por realizar a junção entre dois dataframes. Estando ambos os rdds na forma de um PairRDD, para executar o `.join()`, basta rodar o código abaixo:\n",
        "```python\n",
        "joinned_rdd = rdd1.join(rdd2)\n",
        "```\n",
        "\n",
        "O comando `.join()`por si só executa o conceito do `innerJoin`. Existem também os demais tipos: `fullOuterJoin`, `leftOuterJoin`, `rightOuterJoin` e `cartesian`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53VVNx0NM5VD"
      },
      "source": [
        "### Veja mais\n",
        "É possível encontrar uma lista de todas as transformações disponíveis para uso nos RDDs na API do Spark: http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPpwo2MjNrAT"
      },
      "source": [
        "# Exercícios\n",
        "Para iniciar os exercícios, execute o paragrafo abaixo para que o nosso dataset words esteja disponível para ser utilizado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h21Jv6kHNsMT"
      },
      "source": [
        "text = \"At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures. A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only added to, such as counters and sums\"\n",
        "words = text.split(\" \")\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggBXqqgUsw_y",
        "outputId": "000a6230-9300-4e90-8098-1054be43a187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rdd = sc.parallelize(words, 4)\n",
        "rdd.setName(\"palavras\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "palavras ParallelCollectionRDD[26] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wJu28KfN3rs"
      },
      "source": [
        "Para auxiliar na visualização dos resultados, execute o paragrafo abaixo para definir a função display_df():\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWF7fhyZNzb7",
        "outputId": "b7ade0af-adc6-4d11-9704-743095462813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rdd \\\n",
        "    .map(lambda x: (x,)) \\\n",
        "    .toDF() \\\n",
        "    #.show()\n",
        "\n",
        "def display_rdd(rdd, n=20):\n",
        "    return rdd.toDF().limit(n).toPandas()\n",
        "display_rdd"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.display_rdd>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv_zD6x9ODVB"
      },
      "source": [
        "1) Retorne a lista de palavras distintas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmL5F5iCOJ5S",
        "outputId": "352f03fc-0caf-417b-b529-6a7a3556927f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "rdd2 = rdd.distinct()\\\n",
        "    .map(lambda x: (x,))\\\n",
        "\n",
        "rdd2.take(5)\n",
        "\n",
        "display_rdd(rdd2)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>At</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>operations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>provides</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Hadoop-supported</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>may</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>operations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>failures.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>default,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>when</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tasks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>different</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>variable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  _1\n",
              "0                 At\n",
              "1                 of\n",
              "2         operations\n",
              "3           provides\n",
              "4                 in\n",
              "5                are\n",
              "6              other\n",
              "7   Hadoop-supported\n",
              "8                 an\n",
              "9                may\n",
              "10       operations.\n",
              "11         failures.\n",
              "12              used\n",
              "13          default,\n",
              "14              when\n",
              "15               set\n",
              "16             tasks\n",
              "17         different\n",
              "18             ships\n",
              "19          variable"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnb9E9XwOPu-"
      },
      "source": [
        "2) Implemente um contador de palavras em lower case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR5gaR3IOPvA",
        "outputId": "0997f046-24c7-4306-9a63-8675897a9f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "rdd3 = rdd.map(lambda x: (x.lower(),1))\\\n",
        "  .reduceByKey(lambda a, b: a+b)\\\n",
        "  .sortBy(lambda x: x[1]* -1)\n",
        "\n",
        "display_rdd(rdd3)\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_1</th>\n",
              "      <th>_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spark</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>and</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>be</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>to</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>that</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>parallel</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>on</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>are</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>used</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>which</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>shared</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>driver</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>is</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>function</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>across</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>can</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          _1  _2\n",
              "0          a  12\n",
              "1         in   9\n",
              "2        the   8\n",
              "3         of   6\n",
              "4      spark   6\n",
              "5        and   5\n",
              "6         be   5\n",
              "7         to   5\n",
              "8       that   4\n",
              "9   parallel   4\n",
              "10        on   4\n",
              "11       are   3\n",
              "12      used   3\n",
              "13     which   3\n",
              "14    shared   3\n",
              "15    driver   3\n",
              "16        is   3\n",
              "17  function   3\n",
              "18    across   3\n",
              "19       can   3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3enFjM8OZ96"
      },
      "source": [
        "3)  Realize um inner join do rdd obtido no exercício anterior com o array abaixo utilizando como chave, a palavra em si:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ6qAYrvOZ97",
        "outputId": "9d1fc008-4c6a-4673-e0cb-c0c9fdfb8fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "weights_words = [\n",
        "    (\"spark\", 1000),\n",
        "    (\"parallel\", 500),\n",
        "    (\"function\", 300),\n",
        "    (\"driver\", 400)\n",
        "]\n",
        "weight_rdd = sc.parallelize(weights_words)\n",
        "\n",
        "rdd4 = rdd3.join(weight_rdd) \\\n",
        "  .map(lambda x: {\"word\": x[0], \"count\": x[1][0], \"weigth\": x[1][1]})\n",
        "\n",
        "display_rdd(rdd4)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:401: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
            "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>weigth</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>500</td>\n",
              "      <td>parallel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>400</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>300</td>\n",
              "      <td>function</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>1000</td>\n",
              "      <td>spark</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   count  weigth      word\n",
              "0      4     500  parallel\n",
              "1      3     400    driver\n",
              "2      3     300  function\n",
              "3      6    1000     spark"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGftcLUKySVI",
        "outputId": "e8a1f79c-5389-4b30-d9eb-1030bc2b3c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "## solucao 1 rdd2 = rdd.distinct()\\\n",
        "  .map(lambda x: (x,))\n",
        "rdd2.take(10)\n",
        "#display_rdd(rdd2)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-108-7032a5f9d9f3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    .map(lambda x: (x,))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlkoiYwM7hJx",
        "outputId": "14816b9c-912c-4b1a-e234-8eb224d6561d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "#PairRDD = (K,V)\n",
        "rdd3 = rdd.map(lambda x: (x.lower(),1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: x[1]*-1) # opcional: ordenação\n",
        " \n",
        "display_rdd(rdd3)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_1</th>\n",
              "      <th>_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spark</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>and</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>be</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>to</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>that</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>parallel</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>on</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>are</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>used</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>which</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>shared</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>driver</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>is</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>function</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>across</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>can</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          _1  _2\n",
              "0          a  12\n",
              "1         in   9\n",
              "2        the   8\n",
              "3         of   6\n",
              "4      spark   6\n",
              "5        and   5\n",
              "6         be   5\n",
              "7         to   5\n",
              "8       that   4\n",
              "9   parallel   4\n",
              "10        on   4\n",
              "11       are   3\n",
              "12      used   3\n",
              "13     which   3\n",
              "14    shared   3\n",
              "15    driver   3\n",
              "16        is   3\n",
              "17  function   3\n",
              "18    across   3\n",
              "19       can   3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iAewVLG7svK",
        "outputId": "3b52f0a3-5a82-414d-a348-35719ca6d5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "weights_words = [\n",
        "    (\"spark\", 1000),\n",
        "    (\"parallel\", 500),\n",
        "    (\"function\", 300),\n",
        "    (\"driver\", 400)\n",
        "]\n",
        " \n",
        "weight_rdd = sc.parallelize(weights_words)\n",
        " \n",
        "rdd4 = rdd3.join(weight_rdd) \\\n",
        "    .map(lambda x: {\"word\": x[0], \"count\": x[1][0], \"weight\": x[1][1]})\n",
        " \n",
        "display_rdd(rdd4)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:401: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
            "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>weight</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>500</td>\n",
              "      <td>parallel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>400</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>300</td>\n",
              "      <td>function</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>1000</td>\n",
              "      <td>spark</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   count  weight      word\n",
              "0      4     500  parallel\n",
              "1      3     400    driver\n",
              "2      3     300  function\n",
              "3      6    1000     spark"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    }
  ]
}