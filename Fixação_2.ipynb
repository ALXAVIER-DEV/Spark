{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fixação 2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Fixa%C3%A7%C3%A3o_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pXEN-M0hfV"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l57w6j1HrcGr"
      },
      "source": [
        "# Reading a CSV from google drive\n",
        "\n",
        "Utilizando o Google Colab, é possível importar os datasets diretamente do Google Drive, sem ter que realizar o upload manual dos mesmos para a instância colab manualmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIlwwr7xENz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LB-SAxzAFr"
      },
      "source": [
        "spark.read\\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"delimiter\", \",\") \\\n",
        "  .csv(\"drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_classes/data/vgsales.csv\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aKclVCoLu0J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE0S_cDewVb0"
      },
      "source": [
        "# Introdução\n",
        "Para as questões quem envolverem manipulação de Dataframes, utilize o dataset winemag-data_first150k.csv:\n",
        "\n",
        "https://www.kaggle.com/zynicide/wine-reviews.\n",
        "\n",
        "Faremos algumas análises utilizando esse dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4tL0ybKwYJD",
        "outputId": "e37568ff-bd31-4bc6-edff-94bf38da6c64"
      },
      "source": [
        "csv_path = \"winemag-data_first150k.csv\"\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"description\", StringType(), True),\n",
        "    StructField(\"designation\", StringType(), True),\n",
        "    StructField(\"points\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"province\", StringType(), True),\n",
        "    StructField(\"region_1\", StringType(), True),\n",
        "    StructField(\"region_2\", StringType(), True),\n",
        "    StructField(\"variety\", StringType(), True),\n",
        "    StructField(\"winery\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read.csv(path=csv_path, header=\"true\", schema=schema)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "| id|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|  0|     US|This tremendous 1...|   Martha's Vineyard|    96|235.0|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  1|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  2|     US|Mac Watson honors...|Special Selected ...|    96| 90.0|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|  3|     US|This spent 20 mon...|             Reserve|    96| 65.0|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "|  4| France|This is the top w...|          La Brûlade|    95| 66.0|          Provence|              Bandol|             null|Provence red blend|Domaine de la Bégude|\n",
            "|  5|  Spain|Deep, dense and p...|           Numanthia|    95| 73.0|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
            "|  6|  Spain|Slightly gritty b...|           San Román|    95| 65.0|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
            "|  7|  Spain|Lush cedary black...|Carodorum Único C...|    95|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  8|     US|This re-named vin...|              Silice|    95| 65.0|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergström|\n",
            "|  9|     US|The producer sour...|Gap's Crown Vineyard|    95| 60.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "| 10|  Italy|Elegance, complex...|  Ronco della Chiesa|    95| 80.0|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
            "| 11|     US|From 18-year-old ...|Estate Vineyard W...|    95| 48.0|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 12|     US|A standout even i...|      Weber Vineyard|    95| 48.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 13| France|This wine is in p...|Château Montus Pr...|    95| 90.0|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
            "| 14|     US|With its sophisti...|      Grace Vineyard|    95|185.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "| 15|     US|First made in 200...|              Sigrid|    95| 90.0|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergström|\n",
            "| 16|     US|This blockbuster,...|     Rainin Vineyard|    95|325.0|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "| 17|  Spain|Nicely oaked blac...|6 Años Reserva Pr...|    95| 80.0|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
            "| 18| France|Coming from a sev...|       Le Pigeonnier|    95|290.0|  Southwest France|              Cahors|             null|            Malbec|  Château Lagrézette|\n",
            "| 19|     US|This fresh and li...|Gap's Crown Vineyard|    95| 75.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B7Bqkf6wUYS"
      },
      "source": [
        "# Questão 1\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir a **média das notas (points) obtidos por cada país (country), ordenado decrescentemente pelas notas**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1U-5F0ewZSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fe2f81-3b8b-44e2-8bba-69cb1a11a8d4"
      },
      "source": [
        "# SparkSQL\n",
        "df.createOrReplaceTempView(\"wines\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "  country,\n",
        "  AVG(points) AS average_points\n",
        "FROM wines\n",
        "GROUP BY country\n",
        "ORDER BY AVG(points) DESC\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------------+\n",
            "|     country|   average_points|\n",
            "+------------+-----------------+\n",
            "|     England|92.88888888888889|\n",
            "|     Austria|89.27814136125654|\n",
            "|      France|88.92595050725325|\n",
            "|     Germany|88.63183673469388|\n",
            "|       Italy|88.41341853035144|\n",
            "|      Canada|88.23979591836735|\n",
            "|    Slovenia|88.23404255319149|\n",
            "|     Morocco|88.16666666666667|\n",
            "|      Turkey|88.09615384615384|\n",
            "|    Portugal|88.05713211802293|\n",
            "|   US-France|             88.0|\n",
            "|     Albania|             88.0|\n",
            "|   Australia|87.89396081599676|\n",
            "|          US|87.81835524206477|\n",
            "|      Serbia|87.71428571428571|\n",
            "|       India|           87.625|\n",
            "| New Zealand|87.55562255049743|\n",
            "|     Hungary|87.32900432900433|\n",
            "| Switzerland|            87.25|\n",
            "|South Africa|87.22542072630647|\n",
            "+------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz4MaQVtxg_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e377a74b-0c46-4ffa-b029-808d02f7d2cd"
      },
      "source": [
        "# Pyspark\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "df.select(\"country\", \"points\") \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .agg(\n",
        "      f.avg(\"points\").alias(\"average_points\")\n",
        "  ) \\\n",
        "  .orderBy(f.col(\"average_points\").desc()) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------------+\n",
            "|     country|   average_points|\n",
            "+------------+-----------------+\n",
            "|     England|92.88888888888889|\n",
            "|     Austria|89.27814136125654|\n",
            "|      France|88.92595050725325|\n",
            "|     Germany|88.63183673469388|\n",
            "|       Italy|88.41341853035144|\n",
            "|      Canada|88.23979591836735|\n",
            "|    Slovenia|88.23404255319149|\n",
            "|     Morocco|88.16666666666667|\n",
            "|      Turkey|88.09615384615384|\n",
            "|    Portugal|88.05713211802293|\n",
            "|   US-France|             88.0|\n",
            "|     Albania|             88.0|\n",
            "|   Australia|87.89396081599676|\n",
            "|          US|87.81835524206477|\n",
            "|      Serbia|87.71428571428571|\n",
            "|       India|           87.625|\n",
            "| New Zealand|87.55562255049743|\n",
            "|     Hungary|87.32900432900433|\n",
            "| Switzerland|            87.25|\n",
            "|South Africa|87.22542072630647|\n",
            "+------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUey4_vD4QGA"
      },
      "source": [
        "# Questão 2\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir as **top 20 vinícola (campo winery) mais caras, juntamente com o país e a província**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg5k5joT4-oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3388d483-824d-420c-ea84-daccfffcabac"
      },
      "source": [
        "# SparkSQL\n",
        "df.createOrReplaceTempView(\"wines\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "  country,\n",
        "  province,\n",
        "  winery,\n",
        "  AVG(price) AS average_price\n",
        "FROM wines\n",
        "GROUP BY country, province, winery\n",
        "ORDER BY average_price DESC\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------+--------------------+------------------+\n",
            "|country|      province|              winery|     average_price|\n",
            "+-------+--------------+--------------------+------------------+\n",
            "|     US|    California|               Blair|            1029.0|\n",
            "| France|      Bordeaux|      Château Latour|             794.4|\n",
            "|  Italy|       Tuscany|             Masseto|             587.5|\n",
            "| France|      Bordeaux|  Château Haut-Brion| 569.0909090909091|\n",
            "| France|      Bordeaux|Château La Missio...| 568.1666666666666|\n",
            "| France|     Champagne|                Krug|            513.25|\n",
            "|     US|    California|     Screaming Eagle|             500.0|\n",
            "| France|      Bordeaux|      Château Ausone|             495.0|\n",
            "| France|      Burgundy|Domaine Perrot-Minot|            421.25|\n",
            "| France|      Bordeaux|Château Lafite Ro...|             406.0|\n",
            "| France|      Bordeaux|     Château Margaux|377.53846153846155|\n",
            "|  Spain|Northern Spain|        Vega Sicilia|             367.5|\n",
            "| France|      Bordeaux|Château Laville H...|             357.5|\n",
            "|  Italy|        Veneto|    Dal Forno Romano|            356.75|\n",
            "|  Spain|Northern Spain|            Contador|             354.0|\n",
            "|     US|    California|              Lokoya|             350.0|\n",
            "| France|     Champagne|               Salon|346.42857142857144|\n",
            "|     US|    California|       Bryant Family|             335.0|\n",
            "|     US|    California|            Yao Ming|             327.6|\n",
            "| France|      Bordeaux|    Château Trotanoy|             325.0|\n",
            "+-------+--------------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeNRGlYG4-of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72370835-5977-4562-af13-8b4eb715f8b4"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "# Pyspark\n",
        "df.select(\"country\", \"province\", \"winery\", \"price\") \\\n",
        "  .groupBy(\"country\", \"province\", \"winery\") \\\n",
        "  .agg(\n",
        "      f.avg(\"price\").alias(\"average_price\")\n",
        "  ) \\\n",
        "  .orderBy(f.col(\"average_price\").desc()) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------+--------------------+------------------+\n",
            "|country|      province|              winery|     average_price|\n",
            "+-------+--------------+--------------------+------------------+\n",
            "|     US|    California|               Blair|            1029.0|\n",
            "| France|      Bordeaux|      Château Latour|             794.4|\n",
            "|  Italy|       Tuscany|             Masseto|             587.5|\n",
            "| France|      Bordeaux|  Château Haut-Brion| 569.0909090909091|\n",
            "| France|      Bordeaux|Château La Missio...| 568.1666666666666|\n",
            "| France|     Champagne|                Krug|            513.25|\n",
            "|     US|    California|     Screaming Eagle|             500.0|\n",
            "| France|      Bordeaux|      Château Ausone|             495.0|\n",
            "| France|      Burgundy|Domaine Perrot-Minot|            421.25|\n",
            "| France|      Bordeaux|Château Lafite Ro...|             406.0|\n",
            "| France|      Bordeaux|     Château Margaux|377.53846153846155|\n",
            "|  Spain|Northern Spain|        Vega Sicilia|             367.5|\n",
            "| France|      Bordeaux|Château Laville H...|             357.5|\n",
            "|  Italy|        Veneto|    Dal Forno Romano|            356.75|\n",
            "|  Spain|Northern Spain|            Contador|             354.0|\n",
            "|     US|    California|              Lokoya|             350.0|\n",
            "| France|     Champagne|               Salon|346.42857142857144|\n",
            "|     US|    California|       Bryant Family|             335.0|\n",
            "|     US|    California|            Yao Ming|             327.6|\n",
            "| France|      Bordeaux|    Château Trotanoy|             325.0|\n",
            "+-------+--------------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9kpj8N5r0E"
      },
      "source": [
        "# Questão 3\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir as **a média de preços para cada faixa de pontuação de 10 em 10**:\n",
        "\n",
        "ex: \n",
        "```\n",
        "| faixa | media_preço |\n",
        "-----------------------\n",
        "|10 ~ 20|      19     |\n",
        "|20 ~ 30|      23     |\n",
        "|...    |      ..     |\n",
        "|90 ~100|      95     |\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LflsKBk5iky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0d1429-4bfd-464b-d71a-758754943a97"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "def get_range(number):\n",
        "  return str(int(number/10)) + '0' + ' ~ ' + str(int(number/10)+1) + '0'\n",
        "\n",
        "get_range_udf = udf(get_range, StringType())\n",
        "spark.udf.register(\"get_range_udf\", get_range, StringType())\n",
        "\n",
        "# SparkSQL\n",
        "df.createOrReplaceTempView(\"wines\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "  get_range_udf(points) as range,\n",
        "  avg(price)\n",
        "FROM wines\n",
        "WHERE points > 0\n",
        "GROUP BY range\n",
        "ORDER BY range\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|    range|        avg(price)|\n",
            "+---------+------------------+\n",
            "|100 ~ 110| 401.5833333333333|\n",
            "|  80 ~ 90|23.770047381361923|\n",
            "| 90 ~ 100| 53.83318009288728|\n",
            "+---------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Md7BVBZ5iky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd71063-47d2-47ce-9910-8ef4ddf8a745"
      },
      "source": [
        "# Pyspark\n",
        "df.select(get_range_udf(\"points\").alias(\"range\"), \"price\") \\\n",
        "  .where(\"points > 0\") \\\n",
        "  .groupBy(\"range\") \\\n",
        "  .agg(\n",
        "      f.avg(\"price\").alias(\"average_price\")\n",
        "  ) \\\n",
        "  .orderBy(f.col(\"range\").desc()) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|    range|     average_price|\n",
            "+---------+------------------+\n",
            "| 90 ~ 100| 53.83318009288728|\n",
            "|  80 ~ 90|23.770047381361923|\n",
            "|100 ~ 110| 401.5833333333333|\n",
            "+---------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvUHmCNK-qME"
      },
      "source": [
        "# Questão 4\n",
        "\n",
        "Utilize os RDDs para realizar a contagem de cada letra dado o texto abaixo, ou seja, ao final, teremos a seguinte estrutura:\n",
        "```\n",
        "('a', 35)\n",
        "('b', 32)\n",
        "('c' 5)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwVLRT5m_Kds"
      },
      "source": [
        "text = \"At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures. A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only added to, such as counters and sums. This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either bin/spark-shell for the Scala shell or bin/pyspark for the Python one.\"\n",
        "list_text = text.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-3VJyYKFgCZ",
        "outputId": "e2f73544-d586-47d1-b3c9-7282e8c02acf"
      },
      "source": [
        "list(\"igor\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'g', 'o', 'r']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-zJWTYAPVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4c9905-4812-470a-f236-7abf457ea08e"
      },
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd = sc.parallelize(list_text)\n",
        "\n",
        "rdd.flatMap(lambda word: list(word)) \\\n",
        "  .map(lambda letter: (letter.lower(), 1)) \\\n",
        "  .reduceByKey(lambda a, b: a + b) \\\n",
        "  .sortBy(lambda pair: pair[0]) \\\n",
        "  .take(20)\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(', 2),\n",
              " (')', 2),\n",
              " (',', 14),\n",
              " ('-', 2),\n",
              " ('.', 11),\n",
              " ('/', 2),\n",
              " (':', 1),\n",
              " ('a', 132),\n",
              " ('b', 20),\n",
              " ('c', 44),\n",
              " ('d', 47),\n",
              " ('e', 128),\n",
              " ('f', 28),\n",
              " ('g', 12),\n",
              " ('h', 47),\n",
              " ('i', 82),\n",
              " ('k', 15),\n",
              " ('l', 63),\n",
              " ('m', 20),\n",
              " ('n', 73)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltwodAhxyB8h"
      },
      "source": [
        "# Questão 5\n",
        "\n",
        "Imagine que você foi contratado como engenheiro de dados para construir uma aplicação que seja capaz de processar dados em near real-time relativos a transações bancárias em um banco X.\n",
        "\n",
        "Basicamente, as trasações ocorrem enquanto o usuário utiliza o app do banco X. A cada transação, esses dados são enviados para um servidor em nuvém que irá recebê-los e encaminhar para o sistema de streaming classificá-los.\n",
        "\n",
        "Esse streaming servirá de base para que uma equipe anti-fraude, do banco X, seja informada sempre que uma transação suspeita ocorrer e tomará ações o mais rápido o possível. Assim sendo, o **streaming não será ativo, ou seja, não tomará ações. Ele apenas informará em real-time sobre uma possível fraude em algum lugar**.\n",
        "\n",
        "**BONUS**: Em paralelo a isso, para que seja possível gerar um streaming preciso, um modelo de machine learning é alimentado diariamente com os históricos dos últimos 5 anos das transações bancárias, consideradas fraudes ou não. Esse modelo é treinado diariamente e o modelo gerado é disponibilizado para o streaming, sempre que possível.\n",
        "\n",
        "Como você desenharia a arquitetura dessa solução? Quais os melhores componentes? Como seria o seu pipeline de dados?\n",
        "\n",
        "Descreva utilizando um desenho. Pode mandar uma foto do desenho (a caneta) ou usar uma ferramenta web para desenhar a arquitura (draw.io). Favor enviar diretamente para o professor no Slack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_AGoIy4f1m5"
      },
      "source": [
        "# Questão 6\n",
        "\n",
        "Descreva o que são **accumulators** e **broadcast variables** e explique quando e porque utilizá-las.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqWKVR9xDLW3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1nP0XNDU5g"
      },
      "source": [
        "# Questão 7\n",
        "\n",
        "Utilizando o dado proveniente do Kafka, já visto em sala de aula, implemente um contador de transações em streaming para cada **conta destino** (campo target). Crie duas versões do streaming:\n",
        "\n",
        "- utilizando Structured Streaming\n",
        "- utilizando Spark Streaming\n",
        "\n",
        "Ex.:\n",
        "```\n",
        "# target, coutn\n",
        "('aZ', 123)\n",
        "('as', 43)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQHwcoCmeSxZ"
      },
      "source": [
        "# Structured Streaming\n",
        "qnt_sources_df = identified_df \\\n",
        "  .groupBy(\"target\") \\\n",
        "  .count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y0N_EehfdAi"
      },
      "source": [
        "query = qnt_sources_df \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"qnt_target\") \\\n",
        "  .start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmin0P2YEMY6"
      },
      "source": [
        "# Spark Streaming\n",
        "def basic_transformation(dks):\n",
        "    dks \\\n",
        "        .map(lambda x: (json.loads(x[1])[\"target\"], 1)) \\\n",
        "        .reduceByKey(lambda a,b: a+b, numPartitions=2) \\\n",
        "        .pprint()\n",
        "\n",
        "basic_transformation(dks)\n",
        "\n",
        "#Starting Spark context\n",
        "ssc.checkpoint(\"checkpoint_streaming\")\n",
        "ssc.start()\n",
        "ssc.awaitTermination()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}