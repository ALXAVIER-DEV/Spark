{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fixação 2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Fixa%C3%A7%C3%A3o_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "outputId": "94620062-5697-4ae1-e9d4-e52f524a8c75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [442 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,688 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,365 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [252 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,781 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.3 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,208 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,130 kB]\n",
            "Fetched 11.1 MB in 3s (3,872 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 58kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=20a0ea82b3b0dd9e9d6a4769f0122bbf24cf305b395a6f8656db075eb243485b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pXEN-M0hfV"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l57w6j1HrcGr"
      },
      "source": [
        "# Reading a CSV from google drive\n",
        "\n",
        "Utilizando o Google Colab, é possível importar os datasets diretamente do Google Drive, sem ter que realizar o upload manual dos mesmos para a instância colab manualmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIlwwr7xENz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LB-SAxzAFr",
        "outputId": "93dc5346-cbff-4f94-9ee5-d84d45d77a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.read\\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"delimiter\", \",\") \\\n",
        "  .csv(\"/content/winemag-data_first150k.csv\") \\\n",
        "  .show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|_c0|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|  0|     US|This tremendous 1...|   Martha's Vineyard|    96|235.0|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  1|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  2|     US|Mac Watson honors...|Special Selected ...|    96| 90.0|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|  3|     US|This spent 20 mon...|             Reserve|    96| 65.0|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "|  4| France|This is the top w...|          La Brûlade|    95| 66.0|          Provence|              Bandol|             null|Provence red blend|Domaine de la Bégude|\n",
            "|  5|  Spain|Deep, dense and p...|           Numanthia|    95| 73.0|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
            "|  6|  Spain|Slightly gritty b...|           San Román|    95| 65.0|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
            "|  7|  Spain|Lush cedary black...|Carodorum Único C...|    95|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  8|     US|This re-named vin...|              Silice|    95| 65.0|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergström|\n",
            "|  9|     US|The producer sour...|Gap's Crown Vineyard|    95| 60.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "| 10|  Italy|Elegance, complex...|  Ronco della Chiesa|    95| 80.0|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
            "| 11|     US|From 18-year-old ...|Estate Vineyard W...|    95| 48.0|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 12|     US|A standout even i...|      Weber Vineyard|    95| 48.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 13| France|This wine is in p...|Château Montus Pr...|    95| 90.0|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
            "| 14|     US|With its sophisti...|      Grace Vineyard|    95|185.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "| 15|     US|First made in 200...|              Sigrid|    95| 90.0|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergström|\n",
            "| 16|     US|This blockbuster,...|     Rainin Vineyard|    95|325.0|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "| 17|  Spain|Nicely oaked blac...|6 Años Reserva Pr...|    95| 80.0|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
            "| 18| France|Coming from a sev...|       Le Pigeonnier|    95|290.0|  Southwest France|              Cahors|             null|            Malbec|  Château Lagrézette|\n",
            "| 19|     US|This fresh and li...|Gap's Crown Vineyard|    95| 75.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aKclVCoLu0J",
        "outputId": "33045541-76e1-4f31-815c-1ae644e6d76f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "spark"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d9faa8cbbf5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fa1c708cac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE0S_cDewVb0"
      },
      "source": [
        "# Introdução\n",
        "Para as questões quem envolverem manipulação de Dataframes, utilize o dataset winemag-data_first150k.csv:\n",
        "\n",
        "https://www.kaggle.com/zynicide/wine-reviews.\n",
        "\n",
        "Faremos algumas análises utilizando esse dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4tL0ybKwYJD",
        "outputId": "8ed842fb-8fbe-472d-a722-89fd179a7990"
      },
      "source": [
        "csv_path = \"winemag-data_first150k.csv\"\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"description\", StringType(), True),\n",
        "    StructField(\"designation\", StringType(), True),\n",
        "    StructField(\"points\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"province\", StringType(), True),\n",
        "    StructField(\"region_1\", StringType(), True),\n",
        "    StructField(\"region_2\", StringType(), True),\n",
        "    StructField(\"variety\", StringType(), True),\n",
        "    StructField(\"winery\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read.csv(path=csv_path, header=\"true\", schema=schema)\n",
        "df.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "| id|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|  0|     US|This tremendous 1...|   Martha's Vineyard|    96|235.0|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  1|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  2|     US|Mac Watson honors...|Special Selected ...|    96| 90.0|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|  3|     US|This spent 20 mon...|             Reserve|    96| 65.0|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "|  4| France|This is the top w...|          La Brûlade|    95| 66.0|          Provence|              Bandol|             null|Provence red blend|Domaine de la Bégude|\n",
            "|  5|  Spain|Deep, dense and p...|           Numanthia|    95| 73.0|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
            "|  6|  Spain|Slightly gritty b...|           San Román|    95| 65.0|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
            "|  7|  Spain|Lush cedary black...|Carodorum Único C...|    95|110.0|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|  8|     US|This re-named vin...|              Silice|    95| 65.0|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergström|\n",
            "|  9|     US|The producer sour...|Gap's Crown Vineyard|    95| 60.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "| 10|  Italy|Elegance, complex...|  Ronco della Chiesa|    95| 80.0|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
            "| 11|     US|From 18-year-old ...|Estate Vineyard W...|    95| 48.0|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 12|     US|A standout even i...|      Weber Vineyard|    95| 48.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| 13| France|This wine is in p...|Château Montus Pr...|    95| 90.0|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
            "| 14|     US|With its sophisti...|      Grace Vineyard|    95|185.0|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "| 15|     US|First made in 200...|              Sigrid|    95| 90.0|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergström|\n",
            "| 16|     US|This blockbuster,...|     Rainin Vineyard|    95|325.0|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "| 17|  Spain|Nicely oaked blac...|6 Años Reserva Pr...|    95| 80.0|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
            "| 18| France|Coming from a sev...|       Le Pigeonnier|    95|290.0|  Southwest France|              Cahors|             null|            Malbec|  Château Lagrézette|\n",
            "| 19|     US|This fresh and li...|Gap's Crown Vineyard|    95| 75.0|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "+---+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B7Bqkf6wUYS"
      },
      "source": [
        "# Questão 1\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir a **média das notas (points) obtidos por cada país (country), ordenado decrescentemente pelas notas**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1U-5F0ewZSX",
        "outputId": "2c76c84c-ac29-4112-a39e-aab9848c332b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# SparkSQL\n",
        "df.createOrReplaceView(\"wines\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT \n",
        "  country,\n",
        "  AVG(points) as Media\n",
        "  FROM wines\n",
        "  GROUP by country\n",
        "  Media DESC\n",
        "\n",
        "\"\"\").show()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-edfcbedb1fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SparkSQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m spark.sql(\"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'createOrReplaceView'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz4MaQVtxg_j",
        "outputId": "2726a227-a1f2-427d-b72c-707cd8a6bd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Pyspark\n",
        "import pyspark.sql.functions as f\n",
        "df.select(\"country\", \"points\") \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .agg(\n",
        "      f.avg(\"points\").desc()) \\\n",
        "  .show()\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c95552b2253d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m df.select(\"country\", \"points\")   .groupBy(\"country\")   .agg(\n\u001b[0m\u001b[1;32m      4\u001b[0m       f.avg(\"points\").desc()) \\\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUey4_vD4QGA"
      },
      "source": [
        "# Questão 2\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir as **top 20 vinícola (campo winery) mais caras, juntamente com o país e a província**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg5k5joT4-oe",
        "outputId": "c4d9ff6e-bde2-4e4e-f081-8fa1c4a743c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# SparkSQL\n",
        "\n",
        "df.createOrReplaceView(\"wines\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT \n",
        "    country,\n",
        "    province,\n",
        "    winery,\n",
        "    MAX(price) as maior_preco\n",
        "  FROM wines\n",
        "  GROUP by country\n",
        "  ORDER BY maior_preco DESC\n",
        "\n",
        "\"\"\").show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-13b91ae58f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SparkSQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m spark.sql(\"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'createOrReplaceView'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeNRGlYG4-of"
      },
      "source": [
        "# Pyspark\n",
        "df.select(\"country\",\"province\", \"winery\", \"price\") \\\n",
        "  .groupBy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9kpj8N5r0E"
      },
      "source": [
        "# Questão 3\n",
        "Escreva um conjunto de transformações em Pyspark API e SparkSQL seguido de uma ação `.show()` para exibir as **a média de preços para cada faixa de pontuação de 10 em 10**:\n",
        "\n",
        "ex: \n",
        "```\n",
        "| faixa | media_preço |\n",
        "-----------------------\n",
        "|10 ~ 20|      19     |\n",
        "|20 ~ 30|      23     |\n",
        "|...    |      ..     |\n",
        "|90 ~100|      95     |\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LflsKBk5iky",
        "outputId": "9964589b-b390-4e93-9dd2-d9ea58ab81db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "(11) -> ('10 ~ 20')\n",
        "(17) -> ('10 ~ 20')\n",
        "(89) -> ('80 ~ 90')\n",
        "(97) -> ('90 ~ 100')\n",
        "\n",
        "(11) -> ('10')\n",
        "(17) -> ('10')\n",
        "(89) -> ('80')\n",
        "(97) -> ('90')\n",
        "\n",
        "selectExpr(\"concat(cast(points as string)[0], 0)\")\n",
        "\n",
        "\n",
        "# SparkSQL"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-da72d2de040b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (11) -> ('10 ~ 20')\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Md7BVBZ5iky"
      },
      "source": [
        "# Pyspark\n",
        "def get_range(value):\n",
        "  return str(int(number/10)) + '0' + \n",
        "\n",
        "\n",
        "  df.createGlobalTempView(\"wines\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvUHmCNK-qME"
      },
      "source": [
        "# Questão 4\n",
        "\n",
        "Utilize os RDDs para realizar a contagem de cada letra dado o texto abaixo, ou seja, ao final, teremos a seguinte estrutura:\n",
        "```\n",
        "('a', 35)\n",
        "('b', 32)\n",
        "('c' 5)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwVLRT5m_Kds"
      },
      "source": [
        "text = \"At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures. A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only added to, such as counters and sums. This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either bin/spark-shell for the Scala shell or bin/pyspark for the Python one.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-zJWTYAPVq"
      },
      "source": [
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltwodAhxyB8h"
      },
      "source": [
        "# Questão 5\n",
        "\n",
        "Imagine que você foi contratado como engenheiro de dados para construir uma aplicação que seja capaz de processar dados em near real-time relativos a transações bancárias em um banco X.\n",
        "\n",
        "Basicamente, as trasações ocorrem enquanto o usuário utiliza o app do banco X. A cada transação, esses dados são enviados para um servidor em nuvém que irá recebê-los e encaminhar para o sistema de streaming classificá-los.\n",
        "\n",
        "Esse streaming servirá de base para que uma equipe anti-fraude, do banco X, seja informada sempre que uma transação suspeita ocorrer e tomará ações o mais rápido o possível. Assim sendo, o **streaming não será ativo, ou seja, não tomará ações. Ele apenas informará em real-time sobre uma possível fraude em algum lugar**.\n",
        "\n",
        "**BONUS**: Em paralelo a isso, para que seja possível gerar um streaming preciso, um modelo de machine learning é alimentado diariamente com os históricos dos últimos 5 anos das transações bancárias, consideradas fraudes ou não. Esse modelo é treinado diariamente e o modelo gerado é disponibilizado para o streaming, sempre que possível.\n",
        "\n",
        "Como você desenharia a arquitetura dessa solução? Quais os melhores componentes? Como seria o seu pipeline de dados?\n",
        "\n",
        "Descreva utilizando um desenho. Pode mandar uma foto do desenho (a caneta) ou usar uma ferramenta web para desenhar a arquitura (draw.io). Favor enviar diretamente para o professor no Slack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_AGoIy4f1m5"
      },
      "source": [
        "# Questão 6\n",
        "\n",
        "Descreva o que são **accumulators** e **broadcast variables** e explique quando e porque utilizá-las.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqWKVR9xDLW3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1nP0XNDU5g"
      },
      "source": [
        "# Questão 7\n",
        "\n",
        "Utilizando o dado proveniente do Kafka, já visto em sala de aula, implemente um contador de transações em streaming para cada **conta destino** (campo target). Crie duas versões do streaming:\n",
        "\n",
        "- utilizando Structured Streaming\n",
        "- utilizando Spark Streaming\n",
        "\n",
        "Ex.:\n",
        "```\n",
        "# target, coutn\n",
        "('aZ', 123)\n",
        "('as', 43)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2eizsYJ4i5i"
      },
      "source": [
        "# Structured Streaming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmin0P2YEMY6"
      },
      "source": [
        "# Spark Streaming"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}