{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Aula 6 - Outputs em RDDs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Aula_6_Outputs_em_RDDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pXEN-M0hfV"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l57w6j1HrcGr"
      },
      "source": [
        "# Reading a CSV from google drive\n",
        "\n",
        "Utilizando o Google Colab, é possível importar os datasets diretamente do Google Drive, sem ter que realizar o upload manual dos mesmos para a instância colab manualmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIlwwr7xENz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LB-SAxzAFr"
      },
      "source": [
        "spark.read\\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"delimiter\", \",\") \\\n",
        "  .csv(\"drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_test/test.csv\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNI-rGsPSKSx"
      },
      "source": [
        "# Outputs em RDDs\n",
        "Como visto nas aulas anteriores, uma output é uma **ação** que delimita um **job** e dá origem de fato a execução das **transformações** previamente encadeadas. Essa operação pode ser realizada de duas formas:\n",
        "1. Trazendo dados para o driver da aplicação\n",
        "2. Salvando dados em uma fonte externa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYfUe3kWS6u3"
      },
      "source": [
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhLyTjhCSKSx"
      },
      "source": [
        "### Coleção a ser utilizada\n",
        "Durante essa parte do curso, veremos como aplicar os comandos utilizando a seguinte coleção em Python (texto extraído da seção Overwiew em http://spark.apache.org/docs/latest/rdd-programming-guide.html#overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC5SXvgISKSy"
      },
      "source": [
        "text = \"At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures. A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only added to, such as counters and sums\"\n",
        "words = text.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXicReBaSKS0"
      },
      "source": [
        "words_rdd = sc.parallelize(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENE_o2hmSKS3"
      },
      "source": [
        "### Outputs/ações como estruturas em Python\n",
        "##### rdd.reduce()\n",
        "Reduz um RDD de qualquer tipo a um único número. Por exemplo, uma soma de números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Uo-WcgG3SKS3",
        "outputId": "54d9fb63-6086-4303-b3cf-dcd1b52c28b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "array = range(1, 21)\n",
        "\n",
        "sc.parallelize(array) \\\n",
        "    .reduce(lambda x, y: x + y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbIiQ4s2SKS6"
      },
      "source": [
        "Ou retornar a maior palavra no nosso RDD de words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "P5EEI72WSKS6",
        "outputId": "40e37e75-bb5c-487d-93de-04194fc22802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(\"spark\", \"hadoop\", \"elasticsearch\")\n",
        "(\"hadoop\", \"elasticsearch\")\n",
        "(\"elasticsearch\")\n",
        "\n",
        "\n",
        "def word_leght_reducer(left_word, right_word):\n",
        "  if len(left_word) > len(right_word):\n",
        "    return left_word\n",
        "  else:\n",
        "    return right_word\n",
        "\n",
        "words_rdd.reduce(word_leght_reducer)\n",
        "\n",
        "words_rdd.reduce(lambda x, y: word_leght_reducer(x, y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hadoop-supported'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0XU3i9hSKS9"
      },
      "source": [
        "##### rdd.count()\n",
        "Conta a quantidade de linhas. Além do `.count()`, também há `.max()` e `.min()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EAhjlDfgSKS9",
        "outputId": "9042ffde-c91c-4aa0-d43e-b75a329032de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_rdd.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Rkk5gESKS_"
      },
      "source": [
        "##### rdd.take()\n",
        "Retorna uma quantidade N de linhas do RDD em uma lista em Python no driver. Variações são `.takeOrdered()`, `.takeSample()` e `.top()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "88bdMB89SKTA"
      },
      "source": [
        "words_rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ94p2v6SKTC"
      },
      "source": [
        "##### rdd.collect()\n",
        "Retorna todo o RDD como uma lista de elementos para o driver. Os mesmos cuidados ao se tormar com o `.collect()` em Dataframes são mais do que válidos aqui também"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "n67RtfFrSKTD"
      },
      "source": [
        "#words_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRR83_nSSKTF"
      },
      "source": [
        "### Outputs/ações em Arquivos\n",
        "##### rdd.saveAsTextFile()\n",
        "O comando `.saveAsTextFile()` é uma ação que visa persistir o RDD em formato de um arquivo de texto simples (plain-text). Não é possível, utilizando esse método, salvar nos formatos que vimos nas aulas de Dataframes tais como JSON, CSV, Parquet. Tal feito, requer um recurso que veremos em breve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Q3_uzdSKTF"
      },
      "source": [
        "words_rdd \\\n",
        "    .repartition(2) \\\n",
        "    .saveAsTextFile(\"./output/words_txt_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-rlPezxSKTK"
      },
      "source": [
        "!ls ./output/words_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRmg1NhYSKTM"
      },
      "source": [
        "!head -n 5 ./output/words_txt/part-00000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyFpqIrlSKTO"
      },
      "source": [
        "### Output/Ação utilizando o foreach\n",
        "O comando `.foreach()` basicamente tem a função de iterar sobre as linhas de um RDD. Como é uma ação, e não uma transformação, o comando `.foreach()` **não retorna nenhum valor**. É útil quando se está trabalhando com **Accumulators** ou deseja-se escrever o dado em uma fonte externa. Grande parte dos outputs em formatos de arquivos distintos ou bancos de dados utilizados na API high-level dos Dataframes utiliza essa ação como base para construir a API.\n",
        "\n",
        "Como se usar essa ação. Basta definir uma função passá-la como parâmetro para o método foreach():\n",
        "```python\n",
        "def my_funct(row):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "rdd.foreach(my_funct)\n",
        "```\n",
        "\n",
        "CUIDADO: Erros comuns ao utilizar o comando `.foreach()`:\n",
        "\n",
        "1. Atualização de variáveis globais convencionais. Cada executor tem a sua JVM e a variável global definida no driver não estará disponível para os executores, gerando valores inconsistentes:\n",
        "\n",
        "```python\n",
        "counter = 0\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Wrong: Don't do this!!\n",
        "def increment_counter(x):\n",
        "    global counter\n",
        "    counter += x\n",
        "\n",
        "rdd.foreach(increment_counter)\n",
        "print(\"Counter value: \", counter)\n",
        "```\n",
        "\n",
        "2. Printando elementos usando o `foreach`. Assumindo que a execução está ocorrendo em modo cluster, cada executor pode estar em uma máquina distinta e a execução da função print gerará uma escrita em stdout que ocorrerá dentro do executor e não do driver\n",
        "\n",
        "```python\n",
        "rdd.foreach(print)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBPsx39ASKTO"
      },
      "source": [
        "### Veja mais\n",
        "É possível encontrar uma lista de todas as ações disponíveis para uso nos RDDs na API do Spark: http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipua24M-SKTP"
      },
      "source": [
        "words_rdd.foreach(print)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT0fPGnZgwAN",
        "outputId": "257de3d2-e816-40e1-9b9d-b1bda657de78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "counter = 0\n",
        " \n",
        "# Wrong: Don't do this!!\n",
        "def increment_counter(x):\n",
        "    global counter\n",
        "    counter += 1\n",
        " \n",
        "words_rdd.foreach(increment_counter)\n",
        "print(\"Counter value: \", counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter value:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skJefLKqiD5y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}