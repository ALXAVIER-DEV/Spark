{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Aula 3 - Output e Persistência dos Dataframes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALXAVIER-DEV/Spark/blob/master/Aula_3_Output_e_Persist%C3%AAncia_dos_Dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7 and Java 8. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "outputId": "3a8d6bf4-fc42-4529-d2c0-66853bce3fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 56kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=699af8d4debb97ca8ec41350e6b1cd9387dbc234dc994d0de1a204f02cede1a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pXEN-M0hfV",
        "outputId": "ab90e12c-e2bd-40a9-c7be-b1aafd928377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7cf49390aa86:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f722c93d940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l57w6j1HrcGr"
      },
      "source": [
        "# Reading a CSV from google drive\n",
        "\n",
        "Utilizando o Google Colab, é possível importar os datasets diretamente do Google Drive, sem ter que realizar o upload manual dos mesmos para a instância colab manualmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIlwwr7xENz",
        "outputId": "fc327309-76d5-4e2c-e3d0-1565474e2bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LB-SAxzAFr",
        "outputId": "d6d37ab6-1ee8-4385-b398-2d7abc0ca81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.read\\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"delimiter\", \",\") \\\n",
        "  .csv(\"drive/My\\ Drive/My\\ Professional\\ Carrer/Spark\\ course/virtual_classroom/colab_test/test.csv\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|  nome| \"idade\"|\n",
            "+------+--------+\n",
            "|  igor|    27.0|\n",
            "|kamila|    28.0|\n",
            "+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW_UhfwRG89D"
      },
      "source": [
        "# Output e Persistência dos Dataframes\n",
        "Nessa aula, estaremos cobrindo as mais populares formas de se realizar output e persistência nos Dataframes do Spark bem como os modos de inserção suportados pelo mesmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG_qtIYQG89E"
      },
      "source": [
        "### Carregando o dataset videogamesales\n",
        "Dataset a ser utilizado: https://www.kaggle.com/gregorut/videogamesales/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCuB2rcdG89E"
      },
      "source": [
        "data_dir = \"vgsales.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANHnEtPIG89I"
      },
      "source": [
        "#### Lendo os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUHFbzFyG89I"
      },
      "source": [
        "df = spark.read.load(data_dir, format=\"csv\", inferSchema=\"true\", header=\"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsmkRqlZG89L",
        "outputId": "e381f654-6e25-45a4-ece7-578e120aa0d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Rank: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Platform: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Genre: string (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- NA_Sales: double (nullable = true)\n",
            " |-- EU_Sales: double (nullable = true)\n",
            " |-- JP_Sales: double (nullable = true)\n",
            " |-- Other_Sales: double (nullable = true)\n",
            " |-- Global_Sales: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvlsbG5yG89P"
      },
      "source": [
        "# Output / Ações\n",
        "Output ou ações são o que delimitam um Job, que por sua vez, consiste em um conjunto de transformações em um Dataframe encadeadas e tendo sempre na ponta uma ação final. **Algumas ações são passíveis de serem realizadas de forma distribuída nos executores, mas outras, em decorrência de sua natureza, exige a centralização dos dados na entidade coordenadora chamada driver**. Basicamente:\n",
        "1. Toda ação realizada para persistir o Dataframe **pode** ser realizada de forma distribuída direto pelos executores\n",
        "2. Toda ação realizada para trazer Dataframes Spark para estruturas de dados em Python **exigirá a concentração dos dados no driver**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWk-fgXgG89P"
      },
      "source": [
        "# Lazy evaluation\n",
        "Em conjunto com o conceito das **Ações**, temos também o conceito adotado pelo Spark e outras ferramentas de Big Data: **Lazy Evaluation**. É um conceito que define que **todas as transformações encadeadas só serão executadas quando houver uma ação**. Por exemplo, veja o trecho abaixo:\n",
        "```python\n",
        "df2 = df.select(\"col1\", \"col2\") \\\n",
        "    .filter(f.col(\"col2\") < 10) \\\n",
        "    .groupBy(\"col1\") \\\n",
        "    .agg(f.sum(\"col2\"))\n",
        "```\n",
        "\n",
        "Podemos notar 3 transformações: (1) *select*, (2) *filter* e (3) *groupBy + agg*. Nesse momento, é possível notar que nenhuma das transformações até então foram executadas devido ao conceito da *Lazy Evaluation* (outras ferramentas de Big Data também utilizam essa metodologia), ou seja, as transformações só serão de fato executadas quando houver uma ação. Como ainda não executamos nenhuma, as computações ainda não ocorrem de imediato, conforme visto na [Spark UI](192.168.1.7:8080).\n",
        "\n",
        "A computação somente ocorrerá ao executarmos os trechos abaixo:\n",
        "```python\n",
        "df2.show()\n",
        "df2.collect()\n",
        "```\n",
        "\n",
        "Lazy evaluation é interessante porque permite ao Spark, escolher em tempo de execução o melhor plano a ser seguido, com as otimizações passíveis de serem aplicadas em cada caso, dado as transformações utilizadas pelo programador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McJjVWVuG89P"
      },
      "source": [
        "### Output simples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EXnT7w4G89Q"
      },
      "source": [
        "##### Printando Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZE6AJarG89Q",
        "outputId": "add39d9e-a624-41ea-b81e-f32d21ccec76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
            "|Rank|                Name|Platform|Year|       Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
            "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
            "|   1|          Wii Sports|     Wii|2006|      Sports| Nintendo|   41.49|   29.02|    3.77|       8.46|       82.74|\n",
            "|   2|   Super Mario Bros.|     NES|1985|    Platform| Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|\n",
            "|   3|      Mario Kart Wii|     Wii|2008|      Racing| Nintendo|   15.85|   12.88|    3.79|       3.31|       35.82|\n",
            "|   4|   Wii Sports Resort|     Wii|2009|      Sports| Nintendo|   15.75|   11.01|    3.28|       2.96|        33.0|\n",
            "|   5|Pokemon Red/Pokem...|      GB|1996|Role-Playing| Nintendo|   11.27|    8.89|   10.22|        1.0|       31.37|\n",
            "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM82MgzAG89S"
      },
      "source": [
        "##### Convertendo todo o Dataframe Spark para estruturas em Python puro\n",
        "Obs.: Cuidado!! Esses comandos podem resultar em falta de memoria no processo driver caso o Dataframe seja muito grande. É recomendado utilizá-lo apenas com Dataframes pequenos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsuOkQEFG89T",
        "outputId": "31c51476-bdc6-4018-cb61-0ab4483bd260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.limit(10).collect() # converte o dataframe para uma lista em Python\n",
        "df.limit(10).toPandas() # converte o dataframe Spark para um dataframe Pandas\n",
        "df.limit(10).first() # retorna a primeira linha do Dataframe Spark como uma lista em Python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Rank=1, Name='Wii Sports', Platform='Wii', Year='2006', Genre='Sports', Publisher='Nintendo', NA_Sales=41.49, EU_Sales=29.02, JP_Sales=3.77, Other_Sales=8.46, Global_Sales=82.74)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hfL_0mG89W"
      },
      "source": [
        "##### Contando o número de linhas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OKTvyQXG89X",
        "outputId": "241020f0-d14c-47db-9dd5-37b521a07f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxxvkV_8G89Z"
      },
      "source": [
        "### Output no formato de persistência como uma tabela relacional\n",
        "O Spark, além de ser uma engine de processamento paralelo, possui também um catálogo de dados onde é possível persistir informações de tabelas e utilizar a API do Spark SQL para consultá-las. Para interagir com esse catálogo de dados do Spark, temos duas formas:\n",
        "- Criando tabelas temporárias: `createOrReplaceTempView()`\n",
        "- Criando tabelas definitivas: `df.write.saveAsTable()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6s4C7MGG89a"
      },
      "source": [
        "df.createOrReplaceTempView(\"temp_vgsales\")\n",
        "df.write.saveAsTable(\"stored_vgsales\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_g6ey7LG89c"
      },
      "source": [
        "#### Visualizando as tabelas criadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEPK_3DFG89c",
        "outputId": "4cf05eae-0837-4769-863c-802e39261f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------+-----------+\n",
            "|database|     tableName|isTemporary|\n",
            "+--------+--------------+-----------+\n",
            "| default|stored_vgsales|      false|\n",
            "|        |  temp_vgsales|       true|\n",
            "+--------+--------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xWPU2NhG89f"
      },
      "source": [
        "### Output no formato de persistência em arquivos\n",
        "Ao persistir dataframes em arquivos, o Spark oferece uma gama de possíveis arquivos de saída. Entretando, é necessário ficar atento a alguns pontos sobre os arquivos gerados.\n",
        "\n",
        "1. Para todo output de arquivo gerado, será criado um diretório com o nome escolhido e dentro dele, haverá um ou mais arquivos `part-0000...` contendo os dados propriamente ditos, bem como um arquivo `_SUCCESS` (flag indicando a finalização do processo de persistência do dado)\n",
        "2. É possível controlar a quantidade de partições do arquivo final com o comando `df.coalesce()` ou `df.repartition()`\n",
        "3. Existem 2 modos de realizar o processo de persistência do dado `overwrite` e `append`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdW-qWHZG89f"
      },
      "source": [
        "#### Persistência em arquivo CSV - 1 partição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov51zAfkG89g"
      },
      "source": [
        "# .coalesce(1) - move os dados internamente para UMA partição apenas\n",
        "# .mode(\"overwrite\") - substitui o diretório inteiro caso seja executado novamente\n",
        "# .option(\"header\", \"true\") - incluir o cabeçalho no CSV (específica do formato CSV)\n",
        "\n",
        "df.coalesce(1)\\\n",
        "    .write\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .csv(\"./output/vgsales_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stql85VsG89i",
        "outputId": "fd730cef-4c41-4343-dbf3-2461b8d56c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls output/vgsales_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-9c3a12b7-372c-423a-ac8a-b79ff389d4f2-c000.csv  _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPlB_uWG89k",
        "outputId": "88e799d6-77f0-4b66-f2e4-0e707d74dc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -n 5 output/vgsales_csv/part-00000-9c3a12b7-372c-423a-ac8a-b79ff389d4f2-c000.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rank,Name,Platform,Year,Genre,Publisher,NA_Sales,EU_Sales,JP_Sales,Other_Sales,Global_Sales\n",
            "1,Wii Sports,Wii,2006,Sports,Nintendo,41.49,29.02,3.77,8.46,82.74\n",
            "2,Super Mario Bros.,NES,1985,Platform,Nintendo,29.08,3.58,6.81,0.77,40.24\n",
            "3,Mario Kart Wii,Wii,2008,Racing,Nintendo,15.85,12.88,3.79,3.31,35.82\n",
            "4,Wii Sports Resort,Wii,2009,Sports,Nintendo,15.75,11.01,3.28,2.96,33.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Mgo2DJG89n"
      },
      "source": [
        "#### Persistência em arquivo CSV - 2 partições"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN3DbofJG89o"
      },
      "source": [
        "# .repartition(2) - salva o dataframe utilizando 2 partições\n",
        "\n",
        "df.repartition(2)\\\n",
        "    .write\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .csv(\"./output/vgsales_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1qqsoZ72G89q",
        "outputId": "d7f0711a-8304-46f8-e8be-5764c0263c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls output/vgsales_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-ce3e4050-f78e-4d1b-af75-44225559eb7f-c000.csv  _SUCCESS\n",
            "part-00001-ce3e4050-f78e-4d1b-af75-44225559eb7f-c000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQLxoR5sG89s"
      },
      "source": [
        "#### Persistência em arquivo CSV - 2 partições e modo 'append'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9eZUX2YG89s"
      },
      "source": [
        "# .mode(\"append\") - realiza a operação de append ao salvar no mesmo diretório\n",
        "\n",
        "df.repartition(2)\\\n",
        "    .write\\\n",
        "    .mode(\"append\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .csv(\"./output/vgsales_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "caxfHWBsG89u",
        "outputId": "3d7553fb-3642-4a59-d42a-c97c80ab257d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls output/vgsales_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-a5323aa1-879c-47fc-82ee-8e0a4403c010-c000.csv\n",
            "part-00000-ce3e4050-f78e-4d1b-af75-44225559eb7f-c000.csv\n",
            "part-00001-a5323aa1-879c-47fc-82ee-8e0a4403c010-c000.csv\n",
            "part-00001-ce3e4050-f78e-4d1b-af75-44225559eb7f-c000.csv\n",
            "_SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeMwQ_8dG89x"
      },
      "source": [
        "#### Persistência em arquivo JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9W5qHBRG89x"
      },
      "source": [
        "df.coalesce(1)\\\n",
        "    .write\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .json(\"./output/vgsales_json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O5yVPpkuG89z",
        "outputId": "a4eccf50-5352-4a16-fef2-8f42572006b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls output/vgsales_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-0f205f75-4b8b-4b0d-b6d0-4558bb531a5f-c000.json  _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q3hYZ6eG891",
        "outputId": "e41b66d0-ecc6-4ac6-be3d-a9fb86933db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -n 5 output/vgsales_json/part-00000-0f205f75-4b8b-4b0d-b6d0-4558bb531a5f-c000.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"Rank\":1,\"Name\":\"Wii Sports\",\"Platform\":\"Wii\",\"Year\":\"2006\",\"Genre\":\"Sports\",\"Publisher\":\"Nintendo\",\"NA_Sales\":41.49,\"EU_Sales\":29.02,\"JP_Sales\":3.77,\"Other_Sales\":8.46,\"Global_Sales\":82.74}\n",
            "{\"Rank\":2,\"Name\":\"Super Mario Bros.\",\"Platform\":\"NES\",\"Year\":\"1985\",\"Genre\":\"Platform\",\"Publisher\":\"Nintendo\",\"NA_Sales\":29.08,\"EU_Sales\":3.58,\"JP_Sales\":6.81,\"Other_Sales\":0.77,\"Global_Sales\":40.24}\n",
            "{\"Rank\":3,\"Name\":\"Mario Kart Wii\",\"Platform\":\"Wii\",\"Year\":\"2008\",\"Genre\":\"Racing\",\"Publisher\":\"Nintendo\",\"NA_Sales\":15.85,\"EU_Sales\":12.88,\"JP_Sales\":3.79,\"Other_Sales\":3.31,\"Global_Sales\":35.82}\n",
            "{\"Rank\":4,\"Name\":\"Wii Sports Resort\",\"Platform\":\"Wii\",\"Year\":\"2009\",\"Genre\":\"Sports\",\"Publisher\":\"Nintendo\",\"NA_Sales\":15.75,\"EU_Sales\":11.01,\"JP_Sales\":3.28,\"Other_Sales\":2.96,\"Global_Sales\":33.0}\n",
            "{\"Rank\":5,\"Name\":\"Pokemon Red/Pokemon Blue\",\"Platform\":\"GB\",\"Year\":\"1996\",\"Genre\":\"Role-Playing\",\"Publisher\":\"Nintendo\",\"NA_Sales\":11.27,\"EU_Sales\":8.89,\"JP_Sales\":10.22,\"Other_Sales\":1.0,\"Global_Sales\":31.37}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waEs22BxG893"
      },
      "source": [
        "#### Persistência em arquivo Parquet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHR3jvNYG893"
      },
      "source": [
        "df.coalesce(1)\\\n",
        "    .write\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .parquet(\"./output/vgsales_parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aahp8X2CG895",
        "outputId": "76c4bfdf-4ca7-4bda-ed83-7778af254879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls output/vgsales_parquet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-3cd5293c-e7df-4934-9400-a410fa363b7d-c000.snappy.parquet  _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzLlBw9OG897"
      },
      "source": [
        "### Observações\n",
        "Além de possuir uma gama de formatos de entrada e saída de dados, o Spark também consegue ler e salvar esses dados das mais variadas fontes. Podemos citar algumas tais quais:\n",
        "- Brokers como Kafka, MQTT, AWS Kinesis, etc\n",
        "- Bancos de dados relacionais: MySQL, SQL Server, etc\n",
        "- Cloud storages como S3, ADLS, Blob Storage\n",
        "- Hadoop Distributed File System (HDFS)\n",
        "- Bancos de dados NoSQL: Redis, HBase, Cassandra, MongoDB, Elasticsearch, timeseries DB, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FruMPAB_JYTe"
      },
      "source": [
        "# Exercícios\n",
        "\n",
        "1) Exporte o dataframe vgsales em formato JSON, com 4 partições em modo overwrite ativado no caminho ./output/vgsales_json:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wi7SPusJZhm"
      },
      "source": [
        "# df = ..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}